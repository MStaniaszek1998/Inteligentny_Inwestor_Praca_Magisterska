{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"_file_transforms\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from skainet_spark import Pipeline, ValidatedPipeline, transform, Input, Output, Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    metadata_extracting_disabled = Metadata('/_file_transforms/raw_file_transforms_metadata.csv', spark),\n",
    "    metadata_ddl_extracting_disabled = Metadata('/_file_transforms/ddl_files_metadata.csv', spark),\n",
    "           \n",
    "    ddl_csv_extracting_disabled = Output('/_file_transforms/files_ddl.csv')\n",
    ")\n",
    "def my_function(spark, metadata_extracting_disabled, metadata_ddl_extracting_disabled, ddl_csv_extracting_disabled):\n",
    "    \n",
    "    schema = metadata_extracting_disabled()\n",
    "    df = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .schema(schema)\n",
    "               .load(\"./data/_file_transforms/file_transforms.csv\"))\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline(df)\n",
    "    pipe = (pipe\n",
    "            .where((F.col('extension') == F.lit('parquet')) | \\\n",
    "                   (F.col('transform_type') == F.lit('Metadata')) | \\\n",
    "                   (F.col('stage') == F.lit('ddl'))\n",
    "                   #((F.col('stage') == F.lit('exception')) & (F.col('project').isin(['clubs', 'matches_history'])))\n",
    "                  )\n",
    "            .add_columns({\n",
    "                'type': F.when(F.col('transform_type') == F.lit('Metadata'), F.lit('Schema'))\n",
    "                         .otherwise(\n",
    "                             F.when(F.col('stage') == F.lit('exception'), F.lit('Exceptions'))\n",
    "                              .otherwise(F.lit('Dataset'))\n",
    "                         )\n",
    "            })\n",
    "            .rename_columns({\n",
    "                'path': 'file_id'\n",
    "            })\n",
    "            .select([\n",
    "                'file_id',\n",
    "                'type',\n",
    "                'project',\n",
    "                'stage',\n",
    "                'modified'\n",
    "            ])\n",
    "            .cast_columns({\n",
    "                'modified': 'datetime'\n",
    "            })\n",
    "            .transform(lambda df: df.withColumn('modified', F.date_format(F.col('modified'), 'yyyy-MM-dd HH:mm:ss')))\n",
    "            .distinct()\n",
    "            .transform(deduplicate)\n",
    "            .show_dimensions()\n",
    "           )\n",
    "    \n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata_ddl_extracting_disabled)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv_extracting_disabled)\n",
    "\n",
    "\n",
    "def deduplicate(df):\n",
    "    df = df.withColumn('rank', F.rank().over(Window.partitionBy('file_id').orderBy(F.desc('type'))))\n",
    "    df = df.where(F.col('rank')==1)\n",
    "    df = df.drop('rank')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 5 rows: 100\n",
      "Validated count: 100\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "my_function(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    metadata_extracting_disabled = Metadata('/_file_transforms/raw_file_transforms_metadata.csv', spark),\n",
    "    metadata_ddl_extracting_disabled = Metadata('/_file_transforms/ddl_transforms_metadata.csv', spark),\n",
    "           \n",
    "    ddl_csv_extracting_disabled = Output('/_file_transforms/transforms_ddl.csv')\n",
    ")\n",
    "def my_function(spark, metadata_extracting_disabled, metadata_ddl_extracting_disabled, ddl_csv_extracting_disabled):\n",
    "    \n",
    "    schema = metadata_extracting_disabled()\n",
    "    df = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .schema(schema)\n",
    "               .load(\"./data/_file_transforms/file_transforms.csv\"))\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline(df)\n",
    "    pipe = (pipe\n",
    "            .rename_columns({\n",
    "                'transform_name': 'transform_id'\n",
    "            })\n",
    "            .select([\n",
    "                'transform_id'\n",
    "            ])\n",
    "            .distinct()\n",
    "            .show_dimensions()\n",
    "           )\n",
    "    \n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata_ddl_extracting_disabled)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv_extracting_disabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 1 rows: 23\n",
      "Validated count: 23\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "my_function(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. file_TRANSFORM_transform (INPUT, OUTPUT, METADATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    metadata_extracting_disabled = Metadata('/_file_transforms/raw_file_transforms_metadata.csv', spark),\n",
    "    metadata_ddl_extracting_disabled = Metadata('/_file_transforms/ddl_file_TRANSFORM_transform_metadata.csv', spark),\n",
    "           \n",
    "    ddl_csv_extracting_disabled = Output('/_file_transforms/file_INPUT_transform_ddl.csv')\n",
    ")\n",
    "def my_function(spark, metadata_extracting_disabled, metadata_ddl_extracting_disabled, ddl_csv_extracting_disabled):\n",
    "    \n",
    "    schema = metadata_extracting_disabled()\n",
    "\n",
    "    df = (spark.read\n",
    "           .format(\"csv\")\n",
    "           .option(\"header\", \"true\")\n",
    "           .schema(schema)\n",
    "           .load(\"./data/_file_transforms/file_transforms.csv\"))\n",
    "    \n",
    "    pipe = Pipeline(df)\n",
    "    pipe = (pipe\n",
    "            .where(F.col('transform_type')==F.lit('Input'))\n",
    "            \n",
    "            .where((F.col('extension') == F.lit('parquet')) | \\\n",
    "                   (F.col('transform_type') == F.lit('Metadata')) | \\\n",
    "                   (F.col('stage') == F.lit('ddl'))\n",
    "                  )\n",
    "            .rename_columns({\n",
    "                'path': 'file_id'\n",
    "            })\n",
    "\n",
    "            .rename_columns({\n",
    "                'transform_name': 'transform_id'\n",
    "            })\n",
    "            .select([\n",
    "                'file_id',\n",
    "                'transform_id'\n",
    "            ])\n",
    "            .distinct()\n",
    "            .show_dimensions()\n",
    "           )\n",
    "    \n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata_ddl_extracting_disabled)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "\n",
    "    validated_pipe.write_csv(ddl_csv_extracting_disabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 24\n",
      "Validated count: 24\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "my_function(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    metadata_extracting_disabled = Metadata('/_file_transforms/raw_file_transforms_metadata.csv', spark),\n",
    "    metadata_ddl_extracting_disabled = Metadata('/_file_transforms/ddl_file_TRANSFORM_transform_metadata.csv', spark),\n",
    "           \n",
    "    ddl_csv_extracting_disabled = Output('/_file_transforms/file_OUTPUT_transform_ddl.csv')\n",
    ")\n",
    "def my_function(spark, metadata_extracting_disabled, metadata_ddl_extracting_disabled, ddl_csv_extracting_disabled):\n",
    "    \n",
    "    schema = metadata_extracting_disabled()\n",
    "\n",
    "    df = (spark.read\n",
    "           .format(\"csv\")\n",
    "           .option(\"header\", \"true\")\n",
    "           .schema(schema)\n",
    "           .load(\"./data/_file_transforms/file_transforms.csv\"))\n",
    "    \n",
    "    pipe = Pipeline(df)\n",
    "    pipe = (pipe\n",
    "            .where(F.col('transform_type')==F.lit('Output'))\n",
    "            \n",
    "            .where((F.col('extension') == F.lit('parquet')) | \\\n",
    "                   (F.col('transform_type') == F.lit('Metadata')) | \\\n",
    "                   ((F.col('stage') == F.lit('ddl')) | (F.col('stage') == F.lit('exception')))\n",
    "                  )\n",
    "            .rename_columns({\n",
    "                'path': 'file_id'\n",
    "            })\n",
    "\n",
    "            .rename_columns({\n",
    "                'transform_name': 'transform_id'\n",
    "            })\n",
    "            .select([\n",
    "                'transform_id',\n",
    "                'file_id'\n",
    "            ])\n",
    "            .distinct()\n",
    "            .show_dimensions()\n",
    "           )\n",
    "    \n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata_ddl_extracting_disabled)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "\n",
    "    validated_pipe.write_csv(ddl_csv_extracting_disabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 81\n",
      "Validated count: 81\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "my_function(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    metadata_extracting_disabled = Metadata('/_file_transforms/raw_file_transforms_metadata.csv', spark),\n",
    "    metadata_ddl_extracting_disabled = Metadata('/_file_transforms/ddl_file_TRANSFORM_transform_metadata.csv', spark),\n",
    "           \n",
    "    ddl_csv_extracting_disabled = Output('/_file_transforms/file_METADATA_transform_ddl.csv')\n",
    ")\n",
    "def my_function(spark, metadata_extracting_disabled, metadata_ddl_extracting_disabled, ddl_csv_extracting_disabled):\n",
    "    \n",
    "    schema = metadata_extracting_disabled()\n",
    "\n",
    "    df = (spark.read\n",
    "           .format(\"csv\")\n",
    "           .option(\"header\", \"true\")\n",
    "           .schema(schema)\n",
    "           .load(\"./data/_file_transforms/file_transforms.csv\"))\n",
    "    \n",
    "    pipe = Pipeline(df)\n",
    "    pipe = (pipe\n",
    "            .where(F.col('transform_type')==F.lit('Metadata'))\n",
    "            \n",
    "            .where((F.col('extension') == F.lit('parquet')) | \\\n",
    "                   (F.col('transform_type') == F.lit('Metadata')) | \\\n",
    "                   (F.col('stage') == F.lit('ddl'))\n",
    "                  )\n",
    "            .rename_columns({\n",
    "                'path': 'file_id'\n",
    "            })\n",
    "\n",
    "            .rename_columns({\n",
    "                'transform_name': 'transform_id'\n",
    "            })\n",
    "            .select([\n",
    "                'file_id',\n",
    "                'transform_id'\n",
    "            ])\n",
    "            .distinct()\n",
    "            .show_dimensions()\n",
    "           )\n",
    "    \n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata_ddl_extracting_disabled)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "\n",
    "    validated_pipe.write_csv(ddl_csv_extracting_disabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 41\n",
      "Validated count: 41\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "my_function(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    metadata_extracting_disabled = Metadata('/_file_transforms/raw_file_transforms_metadata.csv', spark),\n",
    "    metadata_ddl_extracting_disabled = Metadata('/_file_transforms/ddl_apps_metadata.csv', spark),\n",
    "           \n",
    "    ddl_csv_extracting_disabled = Output('/_file_transforms/apps_ddl.csv')\n",
    ")\n",
    "def my_function(spark, metadata_extracting_disabled, metadata_ddl_extracting_disabled, ddl_csv_extracting_disabled):\n",
    "    \n",
    "    schema = metadata_extracting_disabled()\n",
    "    df = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .schema(schema)\n",
    "               .load(\"./data/_file_transforms/file_transforms.csv\"))\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline(df)\n",
    "    pipe = (pipe\n",
    "            .select([\n",
    "                'app_id',\n",
    "                'app_name'\n",
    "            ])\n",
    "            .distinct()\n",
    "            .show_dimensions()\n",
    "           )\n",
    "    \n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata_ddl_extracting_disabled)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv_extracting_disabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 11\n",
      "Validated count: 11\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "my_function(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. transform_STAGE_OF_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    metadata_extracting_disabled = Metadata('/_file_transforms/raw_file_transforms_metadata.csv', spark),\n",
    "    metadata_ddl_extracting_disabled = Metadata('/_file_transforms/ddl_transform_STAGE_OF_app_metadata.csv', spark),\n",
    "           \n",
    "    ddl_csv_extracting_disabled = Output('/_file_transforms/transform_STAGE_OF_app_ddl.csv')\n",
    ")\n",
    "def my_function(spark, metadata_extracting_disabled, metadata_ddl_extracting_disabled, ddl_csv_extracting_disabled):\n",
    "    \n",
    "    schema = metadata_extracting_disabled()\n",
    "    df = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .schema(schema)\n",
    "               .load(\"./data/_file_transforms/file_transforms.csv\"))\n",
    "    \n",
    "    \n",
    "    pipe = Pipeline(df)\n",
    "    pipe = (pipe\n",
    "            .where(F.col('stage') != F.lit('exception'))\n",
    "            .rename_columns({\n",
    "                'transform_name': 'transform_id'\n",
    "            })\n",
    "            .select([\n",
    "                'transform_id',\n",
    "                'app_id'\n",
    "            ])\n",
    "            .distinct()\n",
    "            .show_dimensions()\n",
    "           )\n",
    "    \n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata_ddl_extracting_disabled)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv_extracting_disabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 23\n",
      "Validated count: 23\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "my_function(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
