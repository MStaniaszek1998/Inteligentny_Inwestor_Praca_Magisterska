{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "''\n",
    "sc = pyspark.SparkContext(appName=\"company_details_ddl\")\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType,DoubleType,ArrayType,FloatType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skainet_spark import Pipeline, transform, Input, Output, Metadata,ValidatedPipeline,print_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_records(dataframe):\n",
    "    # Remove null records because there are states for UK/Swiss (they aint have states) or any case\n",
    "    dataframe = dataframe.na.drop()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Details - DDL\n",
    "Creating files for Neo4j basing on first source of information - Company Details, that was later divided into distinct datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metdata = Metadata('/company_details/ddl/company_details_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/comp_details_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/comp_details_ddl_exception.csv')\n",
    ")\n",
    "def company_ddl(spark,comp_details_ontology, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_comp_details = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_comp_details = (pipe_comp_details\n",
    "                         .rename_columns({\n",
    "                             \"ticker_symbol\":'company_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'company_id',\n",
    "                             'company_name',\n",
    "                             'website_link',\n",
    "                             'num_employees',\n",
    "                             'description',\n",
    "                             'telephone'\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_comp_details.show_dimensions()\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details.dataframe.show(5)\n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_comp_details, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 6 rows: 418\n",
      "root\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- website_link: string (nullable = true)\n",
      " |-- num_employees: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- telephone: string (nullable = true)\n",
      "\n",
      "+----------+--------------------+--------------------+-------------+--------------------+------------+\n",
      "|company_id|        company_name|        website_link|num_employees|         description|   telephone|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+------------+\n",
      "|       KMI|kinder morgan inc...|http://www.kinder...|        11086|We are one of the...|713-369-9000|\n",
      "|       NEE|nextera energy in...|http://www.nexter...|        14800|NEE is one of the...|561-694-4000|\n",
      "|      FITB| fifth third bancorp|   http://www.53.com|        20182|Fifth Third Banco...|513-921-5505|\n",
      "|       BEN|franklin resource...|http://www.frankl...|         9600|Franklin Resource...|601-939-2545|\n",
      "|       FDX|   fedex corporation|http://www.fedex.com|       500000|FedEx Corporation...|901-818-7500|\n",
      "+----------+--------------------+--------------------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validated count: 418\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "company_ddl(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "     comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/sector_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/sector_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/sector_ddl_exception.csv')\n",
    ")\n",
    "def sector_ddl(spark,comp_details_ontology, metadata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_si = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_si = (pipe_si\n",
    "             .rename_columns({\n",
    "                 \"sector\":'sector_id'\n",
    "             })\n",
    "             .select([\n",
    "                 'sector_id',\n",
    "\n",
    "             ])\n",
    "             .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "            )\n",
    "    pipe_si.show_dimensions()\n",
    "    pipe_si.dataframe.printSchema()\n",
    "    \n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_si, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 1 rows: 16\n",
      "root\n",
      " |-- sector_id: string (nullable = true)\n",
      "\n",
      "Validated count: 16\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "sector_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "     comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/industry_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/industry_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/industry_ddl_exception.csv')\n",
    ")\n",
    "def industry_ddl(spark,comp_details_ontology, metadata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_si = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_si = (pipe_si\n",
    "                         .rename_columns({\n",
    "                             \"industry\":'industry_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'industry_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                                                  .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_si.show_dimensions()\n",
    "    pipe_si.dataframe.printSchema()\n",
    "    \n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_si, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 1 rows: 118\n",
      "root\n",
      " |-- industry_id: string (nullable = true)\n",
      "\n",
      "Validated count: 118\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "industry_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/city_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/city_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/city_ddl_exception.csv')\n",
    ")\n",
    "def city_ddl(spark,comp_details_ontology, metadata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_csc = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_csc = (pipe_csc\n",
    "                         .rename_columns({\n",
    "                             \"city\":'city_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'city_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                                                  .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_csc.show_dimensions()\n",
    "    pipe_csc.dataframe.printSchema()\n",
    "    \n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_csc, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 1 rows: 209\n",
      "root\n",
      " |-- city_id: string (nullable = true)\n",
      "\n",
      "Validated count: 209\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "city_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/state_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/state_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/state_ddl_exception.csv')\n",
    ")\n",
    "def state_ddl(spark,comp_details_ontology, metadata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_csc = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_csc = (pipe_csc\n",
    "                         .rename_columns({\n",
    "                             \"state\":'state_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'state_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_csc.show_dimensions()\n",
    "    pipe_csc.dataframe.printSchema()\n",
    "    \n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_csc, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "\n",
    "def remove_null_records(dataframe):\n",
    "    # Remove null records because there are states for UK/Swiss (they aint have states)\n",
    "    dataframe = dataframe.na.drop()\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 1 rows: 39\n",
      "root\n",
      " |-- state_id: string (nullable = true)\n",
      "\n",
      "Validated count: 39\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "state_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/country_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/country_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/country_ddl_exception.csv')\n",
    ")\n",
    "def country_ddl(spark,comp_details_ontology, metadata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_csc = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_csc = (pipe_csc\n",
    "                         .rename_columns({\n",
    "                             \"country\":'country_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'country_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_csc.show_dimensions()\n",
    "    pipe_csc.dataframe.show(10)\n",
    "    pipe_csc.dataframe.printSchema()\n",
    "    \n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_csc, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "\n",
    "def remove_null_records(dataframe):\n",
    "    # Remove null records because there are states for UK/Swiss (they aint have states)\n",
    "    dataframe = dataframe.na.drop()\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 1 rows: 5\n",
      "+--------------------+\n",
      "|          country_id|\n",
      "+--------------------+\n",
      "|             Ireland|\n",
      "|United States Of ...|\n",
      "|         Switzerland|\n",
      "|                Hm12|\n",
      "|      United Kingdom|\n",
      "+--------------------+\n",
      "\n",
      "root\n",
      " |-- country_id: string (nullable = true)\n",
      "\n",
      "Validated count: 5\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "country_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shareholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/shareholders_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/shareholders_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/shareholders_ddl_exception.csv')\n",
    ")\n",
    "def shareholder_ddl(spark,comp_details_ontology, metadata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_share = Pipeline(comp_details_ontology)\n",
    "    print(metadata)\n",
    "    pipe_share = (pipe_share\n",
    "                         .rename_columns({\n",
    "                             \"shareholders_name\":'shareholder_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'shareholder_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_share.show_dimensions()\n",
    "    pipe_share.dataframe.show(10,0)\n",
    "    pipe_share.dataframe.printSchema()\n",
    "    \n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_share, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "\n",
    "def remove_null_records(dataframe):\n",
    "    # Remove null records because there are states for UK/Swiss (they aint have states)\n",
    "    dataframe = dataframe.na.drop()\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skainet_spark.transform.Metadata object at 0x7fdbb4003c10>\n",
      "cols: 1 rows: 478\n",
      "+-------------------------------------------------+\n",
      "|shareholder_id                                   |\n",
      "+-------------------------------------------------+\n",
      "|Kohlberg Kravis Roberts & Co. LP                 |\n",
      "|Vanguard Windsor Funds - Vanguard Windsor II Fund|\n",
      "|FIL Investments International                    |\n",
      "|Mantle Ridge LP                                  |\n",
      "|First Eagle Investment Management LLC            |\n",
      "|Mairs & Power, Inc.                              |\n",
      "|Fidelity Growth Company Fund                     |\n",
      "|MFS International Intrinsic Value Fund           |\n",
      "|ValueAct Capital Management LP                   |\n",
      "|Elaine P. Wynn                                   |\n",
      "+-------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- shareholder_id: string (nullable = true)\n",
      "\n",
      "Validated count: 478\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "shareholder_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## company_IN_INDUSTRY_industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/company_IN_INDUSTRY_industry_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/company_IN_INDUSTRY_industry_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/company_IN_INDUSTRY_industry_ddl_exception.csv')\n",
    ")\n",
    "def company_IN_INDUSTRY_industry_ddl(spark,comp_details_ontology,metadata, ddl_csv,ddl_csv_exception):\n",
    "    pipe_si = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_si = (pipe_si\n",
    "                         .rename_columns({\n",
    "                             'ticker_symbol':'company_id',\n",
    "                             \"industry\":'industry_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'company_id',\n",
    "                             'industry_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_si.show_dimensions()\n",
    "    pipe_si.dataframe.printSchema()\n",
    "    pipe_si.dataframe.show(10,0)\n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_si, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 436\n",
      "root\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- industry_id: string (nullable = true)\n",
      "\n",
      "+----------+-----------------------------------+\n",
      "|company_id|industry_id                        |\n",
      "+----------+-----------------------------------+\n",
      "|NTRS      |Asset Management                   |\n",
      "|WELL      |REIT—Healthcare Facilities         |\n",
      "|ALL       |Insurance—Property & Casualty      |\n",
      "|PM        |Tobacco                            |\n",
      "|RTX       |Aerospace & Defense                |\n",
      "|HRL       |Packaged Foods                     |\n",
      "|CAT       |Farm & Heavy Construction Machinery|\n",
      "|LOW       |Home Improvement Retail            |\n",
      "|ANSS      |Software—Application               |\n",
      "|RCL       |Travel Services                    |\n",
      "+----------+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Validated count: 436\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "company_IN_INDUSTRY_industry_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## industry_MEMBER_OF_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    " comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/industry_MEMBER_OF_sector_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/industry_MEMBER_OF_sector_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/industry_MEMBER_OF_sector_ddl_exception.csv')\n",
    ")\n",
    "def industry_MEMBER_OF_sector_ddl(spark,comp_details_ontology,metadata, ddl_csv,ddl_csv_exception):\n",
    "    pipe_si = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_si = (pipe_si\n",
    "                         .rename_columns({\n",
    "                             'sector':'sector_id',\n",
    "                             \"industry\":'industry_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'industry_id',\n",
    "                             'sector_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_si.show_dimensions()\n",
    "    pipe_si.dataframe.printSchema()\n",
    "    pipe_si.dataframe.show(10,0)\n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_si, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 119\n",
      "root\n",
      " |-- industry_id: string (nullable = true)\n",
      " |-- sector_id: string (nullable = true)\n",
      "\n",
      "+-------------------------------------+----------------------+\n",
      "|industry_id                          |sector_id             |\n",
      "+-------------------------------------+----------------------+\n",
      "|Industrial Distribution              |Industrials           |\n",
      "|Communication Equipment              |Technology            |\n",
      "|Staffing & Employment Services       |Industrials           |\n",
      "|Specialty Chemicals                  |Basic Materials       |\n",
      "|Telecom Services                     |Communication Services|\n",
      "|Discount Stores                      |Consumer Defensive    |\n",
      "|Utilities—Regulated Electric         |Utilities             |\n",
      "|Medical Devices                      |Healthcare            |\n",
      "|Utilities—Independent Power Producers|Utilities             |\n",
      "|Medical Care Facilities              |Healthcare            |\n",
      "+-------------------------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Validated count: 119\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "industry_MEMBER_OF_sector_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## company_LOCALIZED_IN_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/company_LOCALIZED_IN_city_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/company_LOCALIZED_IN_city_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/company_LOCALIZED_IN_city_ddl_exception.csv')\n",
    ")\n",
    "def company_LOCALIZED_IN_city_ddl(spark,comp_details_ontology,metadata, ddl_csv,ddl_csv_exception):\n",
    "    pipe_csc = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_csc = (pipe_csc\n",
    "                         .rename_columns({\n",
    "                             'ticker_symbol':'company_id',\n",
    "                             \"city\":'city_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'company_id',\n",
    "                             'city_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_csc.show_dimensions()\n",
    "    pipe_csc.dataframe.printSchema()\n",
    "    pipe_csc.dataframe.show(10,0)\n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_csc, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 441\n",
      "root\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- city_id: string (nullable = true)\n",
      "\n",
      "+----------+------------+\n",
      "|company_id|city_id     |\n",
      "+----------+------------+\n",
      "|BSX       |Marlborough |\n",
      "|PPL       |Allentown   |\n",
      "|AMD       |Santa Clara |\n",
      "|VRSK      |Jersey City |\n",
      "|SBAC      |Boca Raton  |\n",
      "|USB       |Minneapolis |\n",
      "|PAYX      |Rochester   |\n",
      "|AEP       |Columbus    |\n",
      "|MDT       |Minneapolis |\n",
      "|TEL       |Schaffhausen|\n",
      "+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Validated count: 441\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "company_LOCALIZED_IN_city_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## city_IS_IN_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/city_IS_IN_state_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/city_IS_IN_state_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/city_IS_IN_state_ddl_exception.csv')\n",
    ")\n",
    "def city_IS_IN_state_ddl(spark,comp_details_ontology,metadata, ddl_csv,ddl_csv_exception):\n",
    "    pipe_csc = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_csc = (pipe_csc\n",
    "                         .rename_columns({\n",
    "                             \"city\":'city_id',\n",
    "                             \"state\":'state_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'city_id',\n",
    "                             'state_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_csc.show_dimensions()\n",
    "    pipe_csc.dataframe.printSchema()\n",
    "    pipe_csc.dataframe.show(10,0)\n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_csc, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 208\n",
      "root\n",
      " |-- city_id: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------+\n",
      "|city_id          |state_id     |\n",
      "+-----------------+-------------+\n",
      "|Cincinnati       |Ohio         |\n",
      "|Pittsburgh       |Pennsylvania |\n",
      "|Columbus         |Ohio         |\n",
      "|Waltham          |Massachusetts|\n",
      "|Newport Beach    |California   |\n",
      "|Livonia          |Michigan     |\n",
      "|Chattanooga      |Tennessee    |\n",
      "|San Jose         |California   |\n",
      "|Minneapolis      |Minnesota    |\n",
      "|Greenwood Village|Colorado     |\n",
      "+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Validated count: 208\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "city_IS_IN_state_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## city_IS_IN_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/city_IS_IN_country_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/city_IS_IN_country_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/city_IS_IN_country_ddl_exception.csv')\n",
    ")\n",
    "def city_IS_IN_country_ddl(spark,comp_details_ontology,metadata, ddl_csv,ddl_csv_exception):\n",
    "    pipe_csc = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_csc = (pipe_csc\n",
    "                         .rename_columns({\n",
    "                             \"city\":'city_id',\n",
    "                             \"country\":'country_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'city_id',\n",
    "                             'country_id',\n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                        .transform(filter_city_in_state)\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_csc.show_dimensions()\n",
    "    pipe_csc.dataframe.printSchema()\n",
    "    pipe_csc.dataframe.show(10,0)\n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_csc, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "def filter_city_in_state(dataframe):\n",
    "    dataframe = dataframe.where(F.col('state').isNull())\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 5\n",
      "root\n",
      " |-- city_id: string (nullable = true)\n",
      " |-- country_id: string (nullable = true)\n",
      "\n",
      "+-------------+--------------+\n",
      "|city_id      |country_id    |\n",
      "+-------------+--------------+\n",
      "|Hertfordshire|United Kingdom|\n",
      "|Dublin       |Hm12          |\n",
      "|Schaffhausen |Switzerland   |\n",
      "|Dublin       |Ireland       |\n",
      "|London       |United Kingdom|\n",
      "+-------------+--------------+\n",
      "\n",
      "Validated count: 5\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "city_IS_IN_country_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_PART_OF_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/state_PART_OF_country_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/state_PART_OF_country_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/state_PART_OF_country_ddl_exception.csv')\n",
    ")\n",
    "def state_PART_OF_country_ddl(spark,comp_details_ontology,metadata, ddl_csv,ddl_csv_exception):\n",
    "    pipe_csc = Pipeline(comp_details_ontology)\n",
    "    #pipe_csc.dataframe.select('country','state','ticker_symbol').where(F.col('country') != F.lit('United States Of America') ).show(100)\n",
    "    pipe_csc = (pipe_csc\n",
    "                         .rename_columns({\n",
    "                             \"state\":'state_id',\n",
    "                             \"country\":'country_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'state_id',\n",
    "                             'country_id',\n",
    "                             \n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                        #.transform(filter_city_in_state)\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_csc.show_dimensions()\n",
    "    pipe_csc.dataframe.printSchema()\n",
    "    pipe_csc.dataframe.show(50,0)\n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_csc, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 2 rows: 39\n",
      "root\n",
      " |-- state_id: string (nullable = true)\n",
      " |-- country_id: string (nullable = true)\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|state_id            |country_id              |\n",
      "+--------------------+------------------------+\n",
      "|Ohio                |United States Of America|\n",
      "|District Of Columbia|United States Of America|\n",
      "|Rhode Island        |United States Of America|\n",
      "|Minnesota           |United States Of America|\n",
      "|Texas               |United States Of America|\n",
      "|Pennsylvania        |United States Of America|\n",
      "|Tennessee           |United States Of America|\n",
      "|Missouri            |United States Of America|\n",
      "|Massachusetts       |United States Of America|\n",
      "|New York            |United States Of America|\n",
      "|North Carolina      |United States Of America|\n",
      "|Georgia             |United States Of America|\n",
      "|Utah                |United States Of America|\n",
      "|Indiana             |United States Of America|\n",
      "|Washington          |United States Of America|\n",
      "|Illinois            |United States Of America|\n",
      "|Louisiana           |United States Of America|\n",
      "|Wisconsin           |United States Of America|\n",
      "|Michigan            |United States Of America|\n",
      "|California          |United States Of America|\n",
      "|Idaho               |United States Of America|\n",
      "|Iowa                |United States Of America|\n",
      "|Connecticut         |United States Of America|\n",
      "|Maine               |United States Of America|\n",
      "|Alabama             |United States Of America|\n",
      "|Kentucky            |United States Of America|\n",
      "|Mississippi         |United States Of America|\n",
      "|Oregon              |United States Of America|\n",
      "|Delaware            |United States Of America|\n",
      "|Colorado            |United States Of America|\n",
      "|Arkansas            |United States Of America|\n",
      "|Nevada              |United States Of America|\n",
      "|Oklahoma            |United States Of America|\n",
      "|Florida             |United States Of America|\n",
      "|Maryland            |United States Of America|\n",
      "|New Jersey          |United States Of America|\n",
      "|Arizona             |United States Of America|\n",
      "|Nebraska            |United States Of America|\n",
      "|Virginia            |United States Of America|\n",
      "+--------------------+------------------------+\n",
      "\n",
      "Validated count: 39\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "state_PART_OF_country_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shareholder_HAS_SHARES_IN_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "comp_details_ontology = Input('/company_details/ontology/company_details_ontology.parquet', spark),\n",
    "\n",
    "    metadata = Metadata('/company_details/ddl/shareholder_HAS_SHARES_IN_company_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/company_details/ddl/shareholder_HAS_SHARES_IN_company_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/company_details/exception/shareholder_HAS_SHARES_IN_company_ddl_exception.csv')\n",
    ")\n",
    "def shareholder_HAS_SHARES_IN_company_ddl(spark,comp_details_ontology,metadata, ddl_csv,ddl_csv_exception):\n",
    "    pipe_sh = Pipeline(comp_details_ontology)\n",
    "    \n",
    "    pipe_sh = (pipe_sh\n",
    "                         .rename_columns({\n",
    "                             \"shareholders_name\":'shareholder_id',\n",
    "                             \"ticker_symbol\":'company_id',\n",
    "                             \"percentage\":'percentage'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'shareholder_id',\n",
    "                             'company_id',\n",
    "                             'percentage'\n",
    "                             \n",
    " \n",
    "                         ])\n",
    "                         .distinct()\n",
    "                        #.transform(filter_city_in_state)\n",
    "                         .transform(remove_null_records)\n",
    "                        )\n",
    "    pipe_sh.show_dimensions()\n",
    "    pipe_sh.dataframe.printSchema()\n",
    "    pipe_sh.dataframe.show(50,0)\n",
    "    schema = metadata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_sh, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 3 rows: 6609\n",
      "root\n",
      " |-- shareholder_id: string (nullable = true)\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- percentage: double (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------+----------+----------+\n",
      "|shareholder_id                                             |company_id|percentage|\n",
      "+-----------------------------------------------------------+----------+----------+\n",
      "|T. Rowe Price Associates, Inc. (Investment Management)     |MET       |1.91      |\n",
      "|Geode Capital Management LLC                               |BRK-B     |1.91      |\n",
      "|Norges Bank (13F)                                          |PEP       |1.0       |\n",
      "|Massachusetts Financial Services Co.                       |CMCSA     |2.97      |\n",
      "|Hotchkis & Wiley Capital Management LLC                    |CMI       |1.6       |\n",
      "|Vanguard 500 Index Fund                                    |GS        |1.89      |\n",
      "|Capital Research & Management Co. (World Investors)        |NKE       |2.59      |\n",
      "|T. Rowe Price Associates, Inc. (Investment Management)     |NKE       |2.23      |\n",
      "|Boston Partners Global Investors, Inc.                     |NBL       |4.51      |\n",
      "|State Street Corp.                                         |HES       |4.22      |\n",
      "|Vanguard 500 Index Fund                                    |NSC       |2.08      |\n",
      "|Capital Research & Management Co. (International Investors)|SLB       |3.88      |\n",
      "|Wellington Management Co. LLP                              |PNC       |5.23      |\n",
      "|Pzena Investment Management, Inc.                          |NWL       |5.94      |\n",
      "|TPG Group Holdings (SBS) Advisors, Inc. (Subfiler)         |IQV       |3.0       |\n",
      "|Vanguard 500 Index Fund                                    |LEN       |2.06      |\n",
      "|State Street Corp.                                         |MO        |4.22      |\n",
      "|Capital Research & Management Co. (World Investors)        |PFE       |3.01      |\n",
      "|BlackRock Fund Advisors                                    |TSCO      |3.03      |\n",
      "|Vanguard Total Stock Market Index Fund                     |TSCO      |2.96      |\n",
      "|Vanguard Total Stock Market Index Fund                     |HSY       |2.9       |\n",
      "|BlackRock Fund Advisors                                    |CRM       |2.16      |\n",
      "|Pershing Square Capital Management LP                      |HLT       |3.73      |\n",
      "|Northern Trust Corp.                                       |WM        |1.21      |\n",
      "|Freefloat                                                  |ARE       |98.7      |\n",
      "|AllianceBernstein LP                                       |AOS       |2.39      |\n",
      "|Freefloat                                                  |DGX       |99.44     |\n",
      "|Vanguard Total Stock Market Index Fund                     |ICE       |2.93      |\n",
      "|T. Rowe Price Associates, Inc. (Investment Management)     |KSU       |4.83      |\n",
      "|Vanguard Mid Cap Index Fund                                |NCLH      |2.55      |\n",
      "|Northern Trust Corp.                                       |VLO       |1.34      |\n",
      "|Vanguard Group, Inc. (Subfiler)                            |MLM       |10.83     |\n",
      "|JPMorgan Investment Management, Inc.                       |SNA       |3.26      |\n",
      "|Managed Account Advisors LLC                               |CB        |2.31      |\n",
      "|The Vanguard Group, Inc.                                   |NOC       |7.89      |\n",
      "|Freefloat                                                  |URI       |99.39     |\n",
      "|T. Rowe Price Associates, Inc. (Investment Management)     |APTV      |10.5      |\n",
      "|PRIMECAP Management Co.                                    |WFC       |1.16      |\n",
      "|Vanguard Group, Inc. (Subfiler)                            |HBI       |11.38     |\n",
      "|Freefloat                                                  |MOS       |90.77     |\n",
      "|Vanguard Total Stock Market Index Fund                     |SWK       |2.75      |\n",
      "|Capital Research & Management Co. (International Investors)|DE        |3.92      |\n",
      "|Vanguard 500 Index Fund                                    |VZ        |2.04      |\n",
      "|BlackRock Fund Advisors                                    |VNO       |2.87      |\n",
      "|Stichting Pensioenfonds ABP (Global Equity Portfolio)      |VNO       |2.76      |\n",
      "|Vanguard 500 Index Fund                                    |TROW      |2.1       |\n",
      "|Vanguard Group, Inc. (Subfiler)                            |EXR       |16.2      |\n",
      "|BlackRock Fund Advisors                                    |V         |2.41      |\n",
      "|Vanguard Group, Inc. (Subfiler)                            |CHRW      |12.92     |\n",
      "|Freefloat                                                  |CMS       |99.51     |\n",
      "+-----------------------------------------------------------+----------+----------+\n",
      "only showing top 50 rows\n",
      "\n",
      "Validated count: 6609\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "shareholder_HAS_SHARES_IN_company_ddl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
