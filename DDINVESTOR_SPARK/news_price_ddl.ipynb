{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "''\n",
    "sc = pyspark.SparkContext(appName=\"news_price_ddl\")\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType,DoubleType,ArrayType,FloatType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skainet_spark import Pipeline, transform, Input, Output, Metadata,ValidatedPipeline,print_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_records(dataframe):\n",
    "    # Remove null records because there are states for UK/Swiss (they aint have states) or any case\n",
    "    dataframe = dataframe.na.drop()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes\n",
    "## Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    news_ontology = Input('/news/ontology/news_ontology.parquet', spark),\n",
    "\n",
    "    metdata = Metadata('/news/ddl/publisher_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/news/ddl/publisher_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/news/exception/publisher_ddl_exception.csv')\n",
    ")\n",
    "def publisher_ddl(spark,news_ontology, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_comp_details = Pipeline(news_ontology)\n",
    "   \n",
    "    pipe_comp_details = (pipe_comp_details\n",
    "                         .rename_columns({\n",
    "                             \"publisher\":'publisher_id'\n",
    "                         })\n",
    "                         .select([\n",
    "                             'publisher_id',\n",
    "\n",
    "\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         .transform(remove_null_records)\n",
    "    )\n",
    "    \n",
    "    pipe_comp_details.show_dimensions()\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details.dataframe.show(5)\n",
    "    \n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_comp_details, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 1 rows: 77\n",
      "root\n",
      " |-- publisher_id: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|        publisher_id|\n",
      "+--------------------+\n",
      "|Markets Insider A...|\n",
      "|Ryan Ermey Associ...|\n",
      "|           Kiplinger|\n",
      "|       MediaPost.com|\n",
      "|           Schaeffer|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validated count: 77\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "publisher_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    news_ontology = Input('/news/ontology/news_ontology.parquet', spark),\n",
    "\n",
    "    metdata = Metadata('/news/ddl/news_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/news/ddl/news_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/news/exception/news_ddl_exception.csv')\n",
    ")\n",
    "def news_ddl(spark,news_ontology, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_comp_details = Pipeline(news_ontology)\n",
    "   \n",
    "    pipe_comp_details = (pipe_comp_details\n",
    "                         .select([\n",
    "                             'news_id',\n",
    "                             'title',\n",
    "                             'content'\n",
    "\n",
    "\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         \n",
    "    )\n",
    "    \n",
    "    pipe_comp_details.show_dimensions()\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details.dataframe.select('news_id').show(5,0)\n",
    "    \n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_comp_details, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 3 rows: 2767\n",
      "root\n",
      " |-- news_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|news_id                                                                                                                                                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|New England Journal of Medicine Publishes Two Positive Phase 3 Trials Showing DUPIXENTÂ® (dupilumab) Improved Moderate-to-Severe Asthma-2018-05-21-PR Newswire                                                           |\n",
      "|Breakfast Technical Briefing on Telecom Services Stocks -- Frontier Communications AT&T Verizon Communications and CenturyLink-2017-06-13-PR Newswire                                                                   |\n",
      "|From the streets of Manhattan to the hills of Hollywood when it comes to using your device when where and how you want Verizon delivers the best experienceOut of 100 metro areas nationwide...-2018-06-18-GlobeNewswire|\n",
      "|Verizon outlines 5G-era growth strategy at investor conference-2019-02-21-GlobeNewswire                                                                                                                                 |\n",
      "|Verizon is building the future for our customers in Florida investing more than $1.4 billion since 2015-2018-08-31-GlobeNewswire                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validated count: 2767\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "news_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    news_ontology = Input('/news/ontology/news_ontology.parquet', spark),\n",
    "    stock_clean = Input('/stock_price/ontology/stock_price_ontology.parquet',spark),\n",
    "    metdata = Metadata('/news/ddl/date_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/news/ddl/date_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/news/exception/date_ddl_exception.csv')\n",
    ")\n",
    "def date_ddl(spark,news_ontology,stock_clean, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_stock = Pipeline(stock_clean)\n",
    "    pipe_news = Pipeline(news_ontology)\n",
    "    \n",
    "    pipe_news = (pipe_news\n",
    "                         .rename_columns({\n",
    "                             \"published_date\":'date_id',\n",
    "                             \n",
    "                         })\n",
    "                         .select([\n",
    "                             'date_id',\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                       \n",
    "    )\n",
    "    pipe_stock = (pipe_stock\n",
    "                         .rename_columns({\n",
    "                             \"open_date\":'date_id',\n",
    "                             \n",
    "                         })\n",
    "                         .select([\n",
    "                             'date_id',\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                       \n",
    "    )\n",
    "    df_union = pipe_stock.dataframe.union(pipe_news.dataframe).orderBy(F.asc('date_id'))\n",
    "    \n",
    "    pipe_union = Pipeline(df_union)\n",
    "    print_statistics(pipe_union)\n",
    " \n",
    "    pipe_union.dataframe.show(5)\n",
    "    \n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_union, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: date (nullable = true)\n",
      "\n",
      "cols: 1 rows: 1020\n",
      "+----------+\n",
      "|   date_id|\n",
      "+----------+\n",
      "|2005-10-06|\n",
      "|2005-10-27|\n",
      "|2005-11-16|\n",
      "|2006-01-23|\n",
      "|2006-03-08|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validated count: 1020\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "date_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    \n",
    "    stock_clean = Input('/stock_price/ontology/stock_price_ontology.parquet',spark),\n",
    "    metdata = Metadata('/stock_price/ddl/stock_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/stock_price/ddl/stock_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/stock_price/exception/stock_ddl_exception.csv')\n",
    ")\n",
    "def stock_ddl(spark,stock_clean, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe = Pipeline(stock_clean)\n",
    "  \n",
    "    \n",
    "    pipe = (pipe\n",
    "            \n",
    "             .select([\n",
    "                 'stock_id',\n",
    "                 'open_price',\n",
    "                 'high_price',\n",
    "                 'low_price',\n",
    "                 'close_price',\n",
    "                 'adj_close_price',\n",
    "                 'volume'\n",
    "             ])\n",
    "             .distinct()\n",
    "                       \n",
    "    )\n",
    "\n",
    " \n",
    "    pipe.dataframe.show(5)\n",
    "    print_statistics(pipe)\n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+---------+-----------+---------------+--------+\n",
      "|      stock_id|open_price|high_price|low_price|close_price|adj_close_price|  volume|\n",
      "+--------------+----------+----------+---------+-----------+---------------+--------+\n",
      "|2020-06-26-BKR|     14.87|     14.94|    14.49|       14.7|           14.7| 6852900|\n",
      "| 2020-06-25-VZ|     53.91| 54.389999|    53.34|  54.279999|      53.677212|17122500|\n",
      "|2020-06-04-HIG| 41.849998| 44.290001|    41.57|  44.279999|      44.279999| 3732800|\n",
      "|2020-07-02-RHI| 52.220001| 53.189999|51.639999|  51.720001|      51.720001|  815000|\n",
      "| 2020-07-08-LB|     15.09|     15.69|    14.86|      15.67|          15.67| 3990900|\n",
      "+--------------+----------+----------+---------+-----------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- stock_id: string (nullable = true)\n",
      " |-- open_price: double (nullable = true)\n",
      " |-- high_price: double (nullable = true)\n",
      " |-- low_price: double (nullable = true)\n",
      " |-- close_price: double (nullable = true)\n",
      " |-- adj_close_price: double (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      "\n",
      "cols: 7 rows: 13641\n",
      "Validated count: 13641\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "stock_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relations\n",
    "## stock_IS_VALUED_FOR_company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    \n",
    "    stock_clean = Input('/stock_price/ontology/stock_price_ontology.parquet',spark),\n",
    "    metdata = Metadata('/stock_price/ddl/stock_IS_VALUED_FOR_company_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/stock_price/ddl/stock_IS_VALUED_FOR_company_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/stock_price/ddl/stock_IS_VALUED_FOR_company_ddl_exception.csv')\n",
    ")\n",
    "def stock_IS_VALUED_FOR_company_ddl(spark,stock_clean, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe = Pipeline(stock_clean)\n",
    "  \n",
    "    \n",
    "    pipe = (pipe\n",
    "             .rename_columns({\n",
    "                 \"ticker_symbol\":'company_id',\n",
    "\n",
    "             })\n",
    "             .select([\n",
    "                 'stock_id',\n",
    "                 'company_id',\n",
    "\n",
    "             ])\n",
    "             .distinct()\n",
    "                       \n",
    "    )\n",
    "\n",
    " \n",
    "    pipe.dataframe.show(5)\n",
    "    print_statistics(pipe)\n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|       stock_id|company_id|\n",
      "+---------------+----------+\n",
      "|2020-06-12-DISH|      DISH|\n",
      "|  2020-06-02-MO|        MO|\n",
      "|   2020-07-02-T|         T|\n",
      "| 2020-07-09-PGR|       PGR|\n",
      "| 2020-07-13-PFG|       PFG|\n",
      "+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- stock_id: string (nullable = true)\n",
      " |-- company_id: string (nullable = true)\n",
      "\n",
      "cols: 2 rows: 13641\n",
      "Validated count: 13641\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "stock_IS_VALUED_FOR_company_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## publisher_PUBLISHES_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    news_ontology = Input('/news/ontology/news_ontology.parquet', spark),\n",
    "\n",
    "    metdata = Metadata('/news/ddl/publisher_PUBLISHES_news_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/news/ddl/publisher_PUBLISHES_news_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/news/exception/publisher_PUBLISHES_news_ddl_exception.csv')\n",
    ")\n",
    "def publisher_PUBLISHES_news_ddl(spark,news_ontology, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_comp_details = Pipeline(news_ontology)\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details = (pipe_comp_details\n",
    "                         .rename_columns({\n",
    "                           \"publisher\":'publisher_id'  \n",
    "                         }).select([\n",
    "                             'publisher_id',\n",
    "                             'news_id'\n",
    "\n",
    "\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         \n",
    "    )\n",
    "    \n",
    "    pipe_comp_details.show_dimensions()\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details.dataframe.show(5)\n",
    "    \n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_comp_details, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- published_date: date (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- mentioned_tickers: string (nullable = true)\n",
      " |-- news_id: string (nullable = true)\n",
      "\n",
      "cols: 2 rows: 2756\n",
      "root\n",
      " |-- publisher_id: string (nullable = true)\n",
      " |-- news_id: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|        publisher_id|             news_id|\n",
      "+--------------------+--------------------+\n",
      "|             Reuters|Note 7 fiasco cou...|\n",
      "|       GlobeNewswire|Verizon to redeem...|\n",
      "|         PR Newswire|John C Lukegord's...|\n",
      "|MT Newswires MTNe...|Stock Futures Def...|\n",
      "|     The Motley Fool|What These 7 Indu...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validated count: 2756\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "publisher_PUBLISHES_news_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## news_CONCERNS_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    news_ontology = Input('/news/ontology/news_ontology.parquet', spark),\n",
    "\n",
    "    metdata = Metadata('/news/ddl/news_CONCERNS_company_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/news/ddl/news_CONCERNS_company_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/news/exception/news_CONCERNS_company_ddl_exception.csv')\n",
    ")\n",
    "def news_CONCERNS_company_ddl(spark,news_ontology, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_comp_details = Pipeline(news_ontology)\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details = (pipe_comp_details\n",
    "                         .rename_columns({\n",
    "                           \"mentioned_tickers\":'company_id'  \n",
    "                         }).select([\n",
    "                             'news_id',\n",
    "                             'company_id'\n",
    "\n",
    "\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         \n",
    "    )\n",
    "    \n",
    "    pipe_comp_details.show_dimensions()\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details.dataframe.show(5)\n",
    "    \n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_comp_details, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- published_date: date (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- mentioned_tickers: string (nullable = true)\n",
      " |-- news_id: string (nullable = true)\n",
      "\n",
      "cols: 2 rows: 5841\n",
      "root\n",
      " |-- news_id: string (nullable = true)\n",
      " |-- company_id: string (nullable = true)\n",
      "\n",
      "+--------------------+----------+\n",
      "|             news_id|company_id|\n",
      "+--------------------+----------+\n",
      "|What Is a Dividen...|        ED|\n",
      "|Why the Markets A...|       LUV|\n",
      "|How Diversificati...|       CVX|\n",
      "|Nasdaq Sees Best ...|      INTC|\n",
      "|Verizon ready to ...|        VZ|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validated count: 5841\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "news_CONCERNS_company_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## news_IS_ISSUED_ON_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    news_ontology = Input('/news/ontology/news_ontology.parquet', spark),\n",
    "\n",
    "    metdata = Metadata('/news/ddl/news_IS_ISSUED_ON_date_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/news/ddl/news_IS_ISSUED_ON_date_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/news/exception/news_IS_ISSUED_ON_date_ddl_exception.csv')\n",
    ")\n",
    "def news_IS_ISSUED_ON_date_ddl(spark,news_ontology, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe_comp_details = Pipeline(news_ontology)\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details = (pipe_comp_details\n",
    "                         .rename_columns({\n",
    "                           \"published_date\":'date_id'  \n",
    "                         }).select([\n",
    "                             'news_id',\n",
    "                             'date_id'\n",
    "\n",
    "\n",
    "                         ])\n",
    "                         .distinct()\n",
    "                         \n",
    "    )\n",
    "    \n",
    "    pipe_comp_details.show_dimensions()\n",
    "    pipe_comp_details.dataframe.printSchema()\n",
    "    pipe_comp_details.dataframe.show(5)\n",
    "    \n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe_comp_details, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- published_date: date (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- mentioned_tickers: string (nullable = true)\n",
      " |-- news_id: string (nullable = true)\n",
      "\n",
      "cols: 2 rows: 2756\n",
      "root\n",
      " |-- news_id: string (nullable = true)\n",
      " |-- date_id: date (nullable = true)\n",
      "\n",
      "+--------------------+----------+\n",
      "|             news_id|   date_id|\n",
      "+--------------------+----------+\n",
      "|10 Small-Cap Stoc...|2020-05-04|\n",
      "|The Zacks Analyst...|2016-09-19|\n",
      "|Verizon is buildi...|2018-09-06|\n",
      "|Yahoo Mail reimag...|2019-09-23|\n",
      "|Verizon is taking...|2017-03-31|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validated count: 2756\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "news_IS_ISSUED_ON_date_ddl(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stock_IS_VALUED_ON_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    \n",
    "    stock_clean = Input('/stock_price/ontology/stock_price_ontology.parquet',spark),\n",
    "    metdata = Metadata('/stock_price/ddl/stock_IS_VALUED_ON_date_metadata_ddl.csv',spark),\n",
    "    ddl_csv = Output('/stock_price/ddl/stock_IS_VALUED_ON_date_ddl.csv'),\n",
    "    ddl_csv_exception = Output('/stock_price/exception/stock_IS_VALUED_ON_date_ddl_exception.csv')\n",
    ")\n",
    "def stock_IS_VALUED_ON_date_ddl(spark,stock_clean, metdata, ddl_csv, ddl_csv_exception):\n",
    "    pipe = Pipeline(stock_clean)\n",
    "  \n",
    "    \n",
    "    pipe = (pipe\n",
    "             .rename_columns({\n",
    "                 \"open_date\":'date_id',\n",
    "\n",
    "             })\n",
    "             .select([\n",
    "                 'stock_id',\n",
    "                 'date_id',\n",
    "\n",
    "             ])\n",
    "             .distinct()\n",
    "                       \n",
    "    )\n",
    "\n",
    " \n",
    "    pipe.dataframe.show(5)\n",
    "    print_statistics(pipe)\n",
    "    schema = metdata()\n",
    "    validated_pipe = ValidatedPipeline(pipe, schema)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "    validated_pipe.write_csv(ddl_csv,ddl_csv_exception)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|       stock_id|   date_id|\n",
      "+---------------+----------+\n",
      "|2020-06-08-CSCO|2020-06-08|\n",
      "| 2020-06-19-HIG|2020-06-19|\n",
      "|2020-06-01-FLIR|2020-06-01|\n",
      "|  2020-06-03-LB|2020-06-03|\n",
      "| 2020-06-10-KEY|2020-06-10|\n",
      "+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- stock_id: string (nullable = true)\n",
      " |-- date_id: date (nullable = true)\n",
      "\n",
      "cols: 2 rows: 13641\n",
      "Validated count: 13641\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "stock_IS_VALUED_ON_date_ddl(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
