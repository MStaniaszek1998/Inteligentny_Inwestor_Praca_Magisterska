{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "''\n",
    "sc = pyspark.SparkContext(appName=\"bi_news_raw_parsed_cleaned\")\n",
    "spark = SparkSession(sc)\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skainet_spark import Pipeline, transform, Input, Output, Metadata,ValidatedPipeline,assign_shortcuts,print_statistics\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType,DoubleType,ArrayType,FloatType, BooleanType,IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi News Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    raw = Output('/bi_news/raw/bi_news_raw.parquet'),\n",
    "    metadata = Metadata('/bi_news/raw/bi_news_metadata_raw.csv', spark)\n",
    ")\n",
    "def raw_bi_news(spark,raw,metadata):\n",
    "    schema=metadata()\n",
    "    df = (spark\n",
    "          .read\n",
    "          .format('json')\n",
    "          \n",
    "          .schema(schema)\n",
    "          .load('input/bi_news_content/*.json')\n",
    "         )\n",
    "    pipeline=Pipeline(df)\n",
    "    pipeline.show_dimensions()\n",
    "    pipeline.dataframe.show(10)\n",
    "    pipeline.write(raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: 5 rows: 1215\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|               title|             content|      published_date|           publisher|ticker_symbol|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|Verizon ends 2019...|\n",
      "4Q 2019 highligh...|Jan. 30, 2020, 07...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|Correcting and Re...|\n",
      "In a release iss...|Aug. 1, 2019, 04:...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|Verizon reports s...|\n",
      "Company sees str...|Aug. 1, 2019, 12:...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|Verizon reports s...|\n",
      "Highest third-qu...|Oct. 25, 2019, 01...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|Strong wireless c...|\n",
      "4Q 2018 highligh...|Jan. 29, 2019, 07...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|Verizon ends firs...|\n",
      "2Q 2018 highligh...|Jul. 24, 2018, 01...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|As 5G era begins,...|\n",
      "3Q 2018 highligh...|Oct. 23, 2018, 01...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|Verizon closes 20...|\n",
      "4Q 2017 highligh...|Jan. 23, 2018, 07...|PRESS RELEASE Glo...|       ['vz']|\n",
      "|Verizon Communica...|\n",
      "NEW YORK, Oct. 2...|Oct. 27, 2005, 01...|PRESS RELEASE PR ...|       ['vz']|\n",
      "|Verizon begins 20...|\n",
      "1Q 2018 highligh...|Apr. 24, 2018, 01...|PRESS RELEASE Glo...|       ['vz']|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_bi_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI News Parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform(spark,\n",
    "    raw = Input('/bi_news/raw/bi_news_raw.parquet',spark),\n",
    "    parsed = Output('/bi_news/parsed/bi_news_parsed.parquet')\n",
    ")\n",
    "def parsed_bi_news(spark,raw,parsed):\n",
    "    pipe = Pipeline(raw)\n",
    "    print_statistics(pipe)\n",
    "    cols_rename = {\n",
    "        'ticker_symbol':'mentioned_tickers'\n",
    "    }\n",
    "    pipe = (pipe\n",
    "            .rename_columns(cols_rename)\n",
    "            .transform(parse_date)\n",
    "            .transform(parse_delete_chars,'title')\n",
    "            .transform(parse_delete_chars,'content')\n",
    "            \n",
    "            .transform(parse_publisher)\n",
    "            #.transform(extract_news_writers)\n",
    "            .transform(drop_records_null_content)\n",
    "            .transform(parse_mentioned_tickers)\n",
    "           )\n",
    "    \n",
    "    print('Business Insider - News')\n",
    "    print_statistics(pipe)\n",
    "    pipe.write(parsed)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def drop_records_null_content(dataframe):\n",
    "    col = 'content'\n",
    "    dataframe = dataframe.where(F.col(col).isNotNull())\n",
    "    return dataframe\n",
    "    \n",
    "\n",
    "def parse_mentioned_tickers(dataframe):\n",
    "    col='mentioned_tickers'\n",
    "    dataframe = dataframe.withColumn(col,F.regexp_replace(F.col(col),'(^\\[)|(\\])',''))\n",
    "    dataframe = dataframe.withColumn(col,F.explode(F.split(F.col(col),',')))\n",
    "    dataframe = dataframe.withColumn(col,F.regexp_replace(F.col(col),'\\.','-'))\n",
    "    dataframe = dataframe.withColumn(col, F.upper(F.col(col)))\n",
    "    return dataframe\n",
    "    \n",
    "def parse_date(dataframe):\n",
    "    \"\"\"date formaat: MMM. d, yyyy, hh:mm a\n",
    "    Example; Jan. 30, 2020, 07:00 AM\"\"\"\n",
    "    col = 'published_date'\n",
    "    dataframe = dataframe.withColumn(col,F.to_date(F.col(col),\"MMM. d, yyyy, hh:mm a\"))\n",
    "    return dataframe\n",
    "    \n",
    "def extract_news_writers(dataframe):\n",
    "    col='publisher'\n",
    "    writ_col= 'writer'\n",
    "    regexp_string = \"\"\"[A-Z][a-z]+ [A-z][a-z]+,\"\"\"\n",
    "    \n",
    "    dataframe = (dataframe\n",
    "                 .withColumn(writ_col,F.when(F.col(col)\n",
    "                                                      .rlike(regexp_string),\n",
    "                                                     F.split(F.col(col),',')[0]).otherwise(F.lit(None).cast(StringType())))\n",
    "                .withColumn(writ_col,F.when(F.size(F.split(F.col(writ_col),' ')) > 2, \n",
    "                                            F.concat_ws(' ',F.split(F.col(writ_col),' ')[0],F.split(F.col(writ_col),' ')[1] )).otherwise(F.col(writ_col))))\n",
    "    \n",
    "    dataframe = dataframe.withColumn(col,F.trim(F.col(col)))\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def parse_delete_chars(dataframe,col):\n",
    "\n",
    "    dataframe = dataframe.withColumn(col,F.regexp_replace(F.col(col),'\\n',' '))\n",
    "    dataframe = dataframe.withColumn(col,F.regexp_replace(F.col(col),\"\\\\s+\",' '))\n",
    "    dataframe = dataframe.withColumn(col,F.regexp_replace(F.col(col),'\\,',''))\n",
    "    dataframe = dataframe.withColumn(col,F.trim(F.col(col)))\n",
    "    return dataframe\n",
    "\n",
    "def parse_publisher(dataframe):\n",
    "    col = 'publisher'\n",
    "    dataframe = dataframe.withColumn(col,F.regexp_replace(F.col(col),'PRESS RELEASE *',''))\n",
    "    format_contributor = {'InvestorPlace':'InvestorPlace','BI Prime':'BI Prime'}\n",
    "    for key in format_contributor:\n",
    "        dataframe = dataframe.withColumn(col,F.when(F.col(col).contains(key),F.lit(format_contributor[key])).otherwise(F.col(col)))\n",
    "    dataframe = dataframe.withColumn(col,F.trim(F.col(col)))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- published_date: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- ticker_symbol: string (nullable = true)\n",
      "\n",
      "cols: 5 rows: 1215\n",
      "Business Insider - News\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- published_date: date (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- mentioned_tickers: string (nullable = true)\n",
      "\n",
      "cols: 5 rows: 1129\n"
     ]
    }
   ],
   "source": [
    "parsed_bi_news(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi news Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@transform(spark,\n",
    "    parsed = Input('/bi_news/parsed/bi_news_parsed.parquet', spark),\n",
    "    metadata = Metadata('/bi_news/clean/bi_news_metadata_clean.csv', spark),\n",
    "    clean = Output('/bi_news/clean/bi_news_clean.parquet'),\n",
    "    clean_exception = Output('/bi_news/exception/bi_news_clean_exception.parquet')\n",
    ")\n",
    "def bi_news_clean(spark, parsed, metadata, clean, clean_exception):\n",
    "    \n",
    "    schema = metadata()\n",
    "    \n",
    "    print(schema.fieldNames())\n",
    "\n",
    "    pipe = Pipeline(parsed)\n",
    "    pipe = (pipe\n",
    "            .show_dimensions()\n",
    "\n",
    "           )\n",
    "    pipe.dataframe.printSchema()\n",
    "   \n",
    "\n",
    "\n",
    "    validated_pipe = ValidatedPipeline(pipe, metadata)\n",
    "    validated_pipe = (validated_pipe\n",
    "                      #.add_validation(F.col('home_team_name') == 'Arsenal', 'column is null')\n",
    "                      .validate()\n",
    "                     )\n",
    "    \n",
    "\n",
    "    validated_pipe.write(clean, clean_exception)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'content', 'published_date', 'publisher', 'mentioned_tickers']\n",
      "cols: 5 rows: 1129\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- published_date: date (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- mentioned_tickers: string (nullable = true)\n",
      "\n",
      "Validated count: 1129\n",
      "Exception count: 0\n"
     ]
    }
   ],
   "source": [
    "bi_news_clean(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
